{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'doc1.txt'}, page_content='Document A'), Document(metadata={'source': 'doc2.txt'}, page_content='Document B')]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     vector_store.add_documents(docs)\n\u001b[32m     33\u001b[39m     time.sleep(delay)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43msafe_add_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36msafe_add_documents\u001b[39m\u001b[34m(vector_store, docs, delay)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_add_documents\u001b[39m(vector_store, docs, delay=\u001b[32m10\u001b[39m):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(docs)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     time.sleep(delay)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:259\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    258\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m msg = (\n\u001b[32m    261\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:620\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    624\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:480\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    484\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KoenFolkers\\dev\\LLM_text_comparison\\venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from helper_funcs import read_txt_file_to_string\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "doc1 = read_txt_file_to_string(\"documents/doc1.txt\")[:10]\n",
    "doc2 = read_txt_file_to_string(\"documents/doc2.txt\")[:10]\n",
    "print(doc1)\n",
    "documents = [\n",
    "    Document(page_content=doc1, metadata={\"source\": \"doc1.txt\"}),\n",
    "    Document(page_content=doc2, metadata={\"source\": \"doc2.txt\"}),\n",
    "]\n",
    "splits = splitter.split_documents(documents)\n",
    "\n",
    "# --- Initialize or load Chroma DB --- https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",\n",
    ")\n",
    "\n",
    "def safe_add_documents(vector_store, docs, delay=10):\n",
    "    print(docs)\n",
    "    vector_store.add_documents(docs)\n",
    "    time.sleep(delay)\n",
    "\n",
    "\n",
    "safe_add_documents(vector_store, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317373fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"criterium\": \"Governance structuur\",\n",
      "        \"overeenstemming\": \"Niet in lijn\",\n",
      "        \"analyse_document_A\": \"Document A biedt geen informatie over de governance structuur.\",\n",
      "        \"analyse_document_B\": \"Document B bevat ook geen details over de governance structuur.\",\n",
      "        \"toelichting\": \"Beide documenten missen essentiële informatie over hoe de governance structuur is opgezet, wat cruciaal is voor compliance met DORA en ISO27001.\",\n",
      "        \"referentie_A\": \"N.v.t.\",\n",
      "        \"referentie_B\": \"N.v.t.\"\n",
      "    },\n",
      "    {\n",
      "        \"criterium\": \"Risicomanagement\",\n",
      "        \"overeenstemming\": \"Niet in lijn\",\n",
      "        \"analyse_document_A\": \"Document A behandelt risicomanagement niet.\",\n",
      "        \"analyse_document_B\": \"Document B biedt ook geen richtlijnen of informatie over risicomanagement.\",\n",
      "        \"toelichting\": \"Zonder een duidelijk risicomanagementbeleid voldoen beide documenten niet aan de vereisten van DORA en ISO27001.\",\n",
      "        \"referentie_A\": \"N.v.t.\",\n",
      "        \"referentie_B\": \"N.v.t.\"\n",
      "    },\n",
      "    {\n",
      "        \"criterium\": \"Incident response\",\n",
      "        \"overeenstemming\": \"Niet in lijn\",\n",
      "        \"analyse_document_A\": \"Document A bevat geen informatie over incident response.\",\n",
      "        \"analyse_document_B\": \"Document B vermeldt ook niets over incident response procedures.\",\n",
      "        \"toelichting\": \"Het ontbreken van incident response plannen in beide documenten is een significante tekortkoming voor compliance.\",\n",
      "        \"referentie_A\": \"N.v.t.\",\n",
      "        \"referentie_B\": \"N.v.t.\"\n",
      "    },\n",
      "    {\n",
      "        \"criterium\": \"Outsourcing beleid\",\n",
      "        \"overeenstemming\": \"Niet in lijn\",\n",
      "        \"analyse_document_A\": \"Document A heeft geen beleid of richtlijnen voor outsourcing.\",\n",
      "        \"analyse_document_B\": \"Document B biedt ook geen informatie over outsourcing beleid.\",\n",
      "        \"toelichting\": \"Zonder een duidelijk outsourcing beleid kunnen beide documenten niet voldoen aan de eisen van DORA en ISO27001.\",\n",
      "        \"referentie_A\": \"N.v.t.\",\n",
      "        \"referentie_B\": \"N.v.t.\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "import json\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Je bent een compliance-expert gespecialiseerd in DORA, ISO27001 en vergelijkbare standaarden.\n",
    "Vergelijk de twee onderstaande documenten per criterium.\n",
    "Geef per criterium:\n",
    "- een samenvatting van hoe elk document het onderwerp behandelt\n",
    "- een beoordeling van de mate van overeenstemming (bijv. \"Volledig in lijn\", \"Gedeeltelijk\", \"Niet in lijn\")\n",
    "- een korte toelichting\n",
    "- citeer indien mogelijk relevante fragmenten uit beide documenten.\n",
    "\n",
    "Document A:\n",
    "\\\"\\\"\\\"{doc1}\\\"\\\"\\\"\n",
    "\n",
    "Document B:\n",
    "\\\"\\\"\\\"{doc2}\\\"\\\"\\\"\n",
    "\n",
    "Criteria:\n",
    "{criteria}\n",
    "\n",
    "Antwoord in geldig JSON-formaat als volgt:\n",
    "[\n",
    "    {{\n",
    "    \"criterium\": \"...\",\n",
    "    \"overeenstemming\": \"...\",\n",
    "    \"analyse_document_A\": \"...\",\n",
    "    \"analyse_document_B\": \"...\",\n",
    "    \"toelichting\": \"...\",\n",
    "    \"referentie_A\": \"...\",\n",
    "    \"referentie_B\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "\"\"\")\n",
    "\n",
    "# 5️⃣ Test data\n",
    "doc1 = \"hi\"\n",
    "doc2 = \"hoi\"\n",
    "criteria = \"Governance structuur\\nRisicomanagement\\nIncident response\\nOutsourcing beleid\"\n",
    "\n",
    "# 6️⃣ Run the chain\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({\n",
    "    \"doc1\": doc1,\n",
    "    \"doc2\": doc2,\n",
    "    \"criteria\": json.dumps(criteria, ensure_ascii=False)\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1b110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
